nameOverride: ""
namespaceOverride: ""
kubeTargetVersionOverride: ""
kubeVersionOverride: ""
fullnameOverride: ""
commonLabels: {}
# scmhash: abc123
# myLabel: aakkmd
crds:
  enabled: true

customRules: {}
  # AlertmanagerFailedReload:
  #   for: 3m
  # AlertmanagerMembersInconsistent:
  #   for: 5m
  #   severity: "warning"

## Create default rules for monitoring the cluster
defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    configReloaders: true
    general: true
    k8s: true
    k8sContainerCpuUsageSecondsTotal: true
    k8sContainerMemoryCache: true
    k8sContainerMemoryRss: true
    k8sContainerMemorySwap: true
    k8sContainerResource: true
    k8sContainerMemoryWorkingSetBytes: true
    k8sPodOwner: true
    kubeApiserverAvailability: true
    kubeApiserverBurnrate: true
    kubeApiserverHistogram: true
    kubeApiserverSlos: true
    kubelet: true
    kubeProxy: true
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    kubeSchedulerAlerting: true
    kubeSchedulerRecording: true
    kubeScheduler: true
    kubeStateMetrics: true
    network: true
    node: true
    nodeExporterAlerting: true
    nodeExporterRecording: true
    prometheus: true
    prometheusOperator: true
    windows: false

  ## Reduce app namespace alert scope
  appNamespacesTarget: ".*"

  ## Set keep_firing_for for all alerts
  keepFiringFor: ""

  ## Labels for default rules
  labels: {}
  ## Annotations for default rules
  annotations: {}

  ## Additional labels for PrometheusRule alerts
  additionalRuleLabels: {}

  ## Additional annotations for PrometheusRule alerts
  additionalRuleAnnotations: {}

  ## Disabled PrometheusRule alerts
  disabled: {}
  # KubeAPIDown: true
  # NodeRAIDDegraded: true

global:
  rbac:
    create: true

    ## Create ClusterRoles that extend the existing view, edit and admin ClusterRoles to interact with prometheus-operator CRDs
    ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles
    createAggregateClusterRoles: false
    pspEnabled: false
    pspAnnotations: {}
      ## Specify pod annotations
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
      ##
      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

  ## Global image registry to use if it needs to be overriden for some specific use cases (e.g local registries, custom images, ...)
  ##
  imageRegistry: ""

  ## Reference to one or more secrets to be used when pulling images
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ##
  imagePullSecrets: []
  # - name: "image-pull-secret"
  # or
  # - "image-pull-secret"

windowsMonitoring:
  ## Deploys the windows-exporter and Windows-specific dashboards and rules (job name must be 'windows-exporter')
  enabled: false

## Configuration for prometheus-windows-exporter
## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-windows-exporter
##
prometheus-windows-exporter:
  ## Enable ServiceMonitor and set Kubernetes label to use as a job label
  ##
  prometheus:
    monitor:
      enabled: true
      jobLabel: jobLabel

  releaseLabel: true

  ## Set job label to 'windows-exporter' as required by the default Prometheus rules and Grafana dashboards
  ##
  podLabels:
    jobLabel: windows-exporter

  ## Enable memory and container metrics as required by the default Prometheus rules and Grafana dashboards
  ##
  config: |-
    collectors:
      enabled: '[defaults],memory,container'


## Configuration for alertmanager
## ref: https://prometheus.io/docs/alerting/alertmanager/
alertmanager:
  fullnameOverride: alertmanager
  alertmanagerSpec:
    replicas: 1
    useExistingSecret: true

    # configMaps:
    #   - alertmanager-telegram-template
    configSecret: alertmanager-secret
    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: alertmanager
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: local-path
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 1Gi

  config:
    receivers:
      - name: empty
      - name: alertmanager-bot
        webhook_configs:
          - send_resolved: true
            url: "http://alertmanager-bot:8080"
    route:
      group_by: ["alertname"]
      receiver: alertmanager-bot
      routes:
        - matchers:
            - alertname=~"etcdInsufficientMembers|Watchdog|KubeProxyDown|KubeSchedulerDown|KubeControllerManagerDown"
          receiver: empty
        - matchers:
            - alertname="TargetDown"
            - job=~"kube-controller-manager|kube-etcd|kube-proxy|kube-scheduler"
          receiver: empty
        - matchers:
            - alertname="PrometheusOperatorRejectedResources"
          receiver: empty

    # resources:
    #   requests:
    #     cpu: 10m
    #     memory: 40Mi
    #   limits:
    #     cpu: 200m
    #     memory: 250Mi

# ===========================================================================================
# Grafana
# Default values: https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
# ===========================================================================================

grafana:
  enabled: true
  #fullnameOverride: grafana
  defaultDashboardsTimezone: cet
  defaultDashboardsEnabled: true
  deploymentStrategy:
    type: Recreate
  revisionHistoryLimit: 4
  serviceMonitor:
    scrapeTimeout: 5s

  # needed for loading initial datasources
  # admin:
  #   existingSecret: grafana-admin-secret
    # userKey: admin-user
    # passwordKey: admin-password

  plugins:
    - grafana-piechart-panel

  persistence:
    enabled: true
    size: 10Gi
    accessModes:
      - ReadWriteOnce
    finalizers:
      - kubernetes.io/pvc-protection

  inMemory:
    enabled: true
    ## The maximum usage on memory medium EmptyDir would be
    ## the minimum value between the SizeLimit specified
    ## here and the sum of memory limits of all containers in a pod
    ##
    sizeLimit: 256Mi


  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          url: http://monitoring-kube-prometheus-prometheus.monitoring:9090 #http://kube-prometheus-stack-prometheus:9090
          access: proxy
          # isDefault: true
        - name: Loki
          type: loki
          url: http://loki-stack:3100
          access: proxy
        # - name: mqtt
        #   type: mqtt
        #   url: tcp://192.168.40.183:1883
        #   access: proxy

  sidecar:
    # resources:
    #   limits:
    #     memory: 128Mi
    #   requests:
    #     cpu: 10m
    #     memory: 64Mi

    # dashboards:
    #   enabled: true
    #   label: grafana_dashboard
    #   resource: configmap
    #   folderAnnotation: grafana.grafana.com/dashboards.target-directory
    #   provider:
    #     foldersFromFilesStructure: true
    #   annotations:
    #     grafana.grafana.com/dashboards.target-directory: "/tmp/dashboards/kubernetes"
    #   searchNamespace: ALL


  # custom-dashboard:
  # #   # This is a path to a file inside the dashboards directory inside the chart directory
  #   file: dashboards/router.json
  #     extra:
  #       pihole-overview:
  #         gnetId: 10176
  #         datasource: Prometheus
  #       argocd-overview:
  #         gnetId: 14584
  #         datasource: Prometheus
  #       cert-manager-overview:
  #         gnetId: 11001
  #         datasource: Prometheus
  #       external-dns-overview:
  #         gnetId: 15038
  #         datasource: Prometheus
  #       blackbox-exporter-overview:
  #         gnetId: 5345
  #         datasource: Prometheus

    # Organise dashboards by provider, with the provider's name as the key.
    dashboards:
      default: # The "default" provider is defined below.
        nginx-ingress-controller:
          url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.48.1/deploy/grafana/dashboards/nginx.json
        ingress-request-performance:
          url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.48.1/deploy/grafana/dashboards/request-handling-performance.json
      extra:
        pihole-overview:
          gnetId: 10176
          datasource: Prometheus
        argocd-overview:
          gnetId: 14584
          datasource: Prometheus
        cert-manager-overview:
          gnetId: 11001
          datasource: Prometheus
        external-dns-overview:
          gnetId: 15038
          datasource: Prometheus
        blackbox-exporter-overview:
          gnetId: 5345
          datasource: Prometheus
        # thanos-overview:
        #   gnetId: 12937
        #   datasource: Thanos

    # Configure dashboard providers.
    # ref: http://docs.grafana.org/administration/provisioning/#dashboards
    # `path` must be /var/lib/grafana/dashboards/<provider_name>
    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
          - name: default
            orgId: 1
            folder: ""
            type: file
            disableDeletion: false
            editable: true
            allowUiUpdates: true
            options:
              path: /var/lib/grafana/dashboards/default
          - name: "extra"
            orgId: 1
            folder: "Extra"
            type: file
            disableDeletion: true
            editable: true
            allowUiUpdates: true
            options:
              path: /var/lib/grafana/dashboards/extra

  #envFromSecret: grafana-secret

  grafana.ini:
    server:
      root_url: https://grafana.anotherlife.pl
    paths:
      data: /var/lib/grafana/
      logs: /var/log/grafana
      plugins: /var/lib/grafana/plugins
      provisioning: /etc/grafana/provisioning
    analytics:
      check_for_updates: true
    log:
      mode: console
      level: debug #info

    # auth.generic_oauth:
    #   enabled: true
    #   allow_sign_up: true
    #   name: idm.anotherlife.pl
    #   client_id: $__env{GRAFANA_SSO_CLIENT_ID}
    #   client_secret: $__env{GRAFANA_SSO_CLIENT_SECRET}
    #   scopes: openid profile email
    #   auth_url: https://idm.anotherlife.pl/ui/oauth2
    #   token_url: https://idm.anotherlife.pl/oauth2/token
    #   api_url: https://idm.anotherlife.pl/oauth2/openid/grafana/userinfo
    #   use_pkce: true
    #   groups_attribute_path: scopes
    #   name_attribute_path: preferred_username
    #   role_attribute_path: contains(scopes[*], 'admin') && 'Admin' || 'Viewer'

  serviceMonitor:
    enabled: true
    path: /metrics
    labels:
      release: monitoring

kubelet:
  enabled: true
  serviceMonitor:
    metricRelabelings:
      - action: replace
        sourceLabels:
          - node
        targetLabel: instance

kubeApiServer:
  enabled: true

coreDns:
  enabled: true
  service:
    selector:
      k8s-app: kube-dns

kubeEtcd:
  enabled: true
  endpoints: # ips of servers
    - 192.168.40.110
    # - 192.168.40.111
    # - 192.168.40.112
  service:
    enabled: true
    port: 2381
    targetPort: 2381

kubeScheduler:
  enabled: true
  endpoints: # ips of servers
    - 192.168.40.110
    # - 192.168.40.111
    # - 192.168.40.112

kubeProxy:
  enabled: true
  endpoints: # ips of servers
    - 192.168.40.110
    # - 192.168.40.111
    # - 192.168.40.112

kube-state-metrics:
  fullnameOverride: kube-state-metrics
  selfMonitor:
    enabled: true
  prometheus:
    monitor:
      enabled: true
      relabelings:
        - action: replace
          regex: (.*)
          replacement: $1
          sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: kubernetes_node

nodeExporter:
  enabled: true
  serviceMonitor:
    relabelings:
      - action: replace
        regex: (.*)
        replacement: $1
        sourceLabels:
          - __meta_kubernetes_pod_node_name
        targetLabel: kubernetes_node


kubeControllerManager:
  enabled: true
  endpoints: # ips of servers
    - 192.168.40.110
    # - 192.168.40.111
    # - 192.168.40.112

prometheusOperator:
  admissionWebhooks:
    enabled: true
  tls:
    enabled: false
  # prometheusConfigReloader:
  #   resources:
  #     requests:
  #       cpu: 200m
  #       memory: 50Mi
  #     limits:
  #       memory: 100Mi

# prometheus-node-exporter:
#   prometheus:
#     monitor:
#       enabled: true
#       relabelings:
#         - action: replace
#           sourceLabels:
#             - __meta_kubernetes_pod_node_name
#           targetLabel: instance
# ## smart disk data metrics
# extraArgs:
#   - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
#   - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
#   # this is the new line
#   - --collector.textfile.directory=/host/root/var/log/prometheus


prometheus-node-exporter:
  fullnameOverride: node-exporter
  podLabels:
    jobLabel: node-exporter
  extraArgs:
    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
    - --collector.textfile.directory=/host/root/var/log/prometheus

  service:
    portName: http-metrics
  prometheus:
    monitor:
      enabled: true
      relabelings:
        - action: replace
          regex: (.*)
          replacement: $1
          sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: kubernetes_node
  # resources:
  #   requests:
  #     memory: 512Mi
  #     cpu: 250m
  #   limits:
  #     memory: 2048Mi

prometheus:
  prometheusSpec:
    replicas: 1
    retentionSize: 3GB
    retention: 7d
    ruleSelectorNilUsesHelmValues: true
    serviceMonitorSelectorNilUsesHelmValues: true
    podMonitorSelectorNilUsesHelmValues: true
    probeSelectorNilUsesHelmValues: true
    scrapeConfigSelectorNilUsesHelmValues: true
    replicaExternalLabelName: "replica"
    enableAdminAPI: true
    walCompression: true
    # resources:
    #   requests:
    #     cpu: 1
    #     memory: 2Gi
    #   limits:
    #     cpu: 4
    #     memory: 2.5Gi
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: local-path #longhorn-retain
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi

