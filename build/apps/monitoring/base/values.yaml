nameOverride: ""
namespaceOverride: ""
kubeTargetVersionOverride: ""
kubeVersionOverride: ""
fullnameOverride: ""
commonLabels: {}
# scmhash: abc123
# myLabel: aakkmd
crds:
  enabled: true

customRules: {}
  # AlertmanagerFailedReload:
  #   for: 3m
  # AlertmanagerMembersInconsistent:
  #   for: 5m
  #   severity: "warning"

## Create default rules for monitoring the cluster
defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    configReloaders: true
    general: true
    k8s: true
    k8sContainerCpuUsageSecondsTotal: true
    k8sContainerMemoryCache: true
    k8sContainerMemoryRss: true
    k8sContainerMemorySwap: true
    k8sContainerResource: true
    k8sContainerMemoryWorkingSetBytes: true
    k8sPodOwner: true
    kubeApiserverAvailability: true
    kubeApiserverBurnrate: true
    kubeApiserverHistogram: true
    kubeApiserverSlos: true
    kubelet: true
    kubeProxy: true
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    kubeSchedulerAlerting: true
    kubeSchedulerRecording: true
    kubeScheduler: true
    kubeStateMetrics: true
    network: true
    node: true
    nodeExporterAlerting: true
    nodeExporterRecording: true
    prometheus: true
    prometheusOperator: true
    windows: false

  ## Reduce app namespace alert scope
  appNamespacesTarget: ".*"

  ## Set keep_firing_for for all alerts
  keepFiringFor: ""

  ## Labels for default rules
  labels: {}
  ## Annotations for default rules
  annotations: {}

  ## Additional labels for PrometheusRule alerts
  additionalRuleLabels: {}

  ## Additional annotations for PrometheusRule alerts
  additionalRuleAnnotations: {}

  ## Disabled PrometheusRule alerts
  disabled: {}
  # KubeAPIDown: true
  # NodeRAIDDegraded: true

global:
  rbac:
    create: true

    ## Create ClusterRoles that extend the existing view, edit and admin ClusterRoles to interact with prometheus-operator CRDs
    ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles
    createAggregateClusterRoles: false
    pspEnabled: false
    pspAnnotations: {}
      ## Specify pod annotations
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
      ##
      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

  ## Global image registry to use if it needs to be overriden for some specific use cases (e.g local registries, custom images, ...)
  ##
  imageRegistry: ""

  ## Reference to one or more secrets to be used when pulling images
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ##
  imagePullSecrets: []
  # - name: "image-pull-secret"
  # or
  # - "image-pull-secret"

windowsMonitoring:
  ## Deploys the windows-exporter and Windows-specific dashboards and rules (job name must be 'windows-exporter')
  enabled: false

## Configuration for prometheus-windows-exporter
## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-windows-exporter
##
prometheus-windows-exporter:
  ## Enable ServiceMonitor and set Kubernetes label to use as a job label
  ##
  prometheus:
    monitor:
      enabled: true
      jobLabel: jobLabel

  releaseLabel: true

  ## Set job label to 'windows-exporter' as required by the default Prometheus rules and Grafana dashboards
  ##
  podLabels:
    jobLabel: windows-exporter

  ## Enable memory and container metrics as required by the default Prometheus rules and Grafana dashboards
  ##
  config: |-
    collectors:
      enabled: '[defaults],memory,container'


## Configuration for alertmanager
## ref: https://prometheus.io/docs/alerting/alertmanager/
alertmanager:
  fullnameOverride: alertmanager
  enabled: true
  annotations: {}
  ## Api that prometheus will use to communicate with alertmanager. Possible values are v1, v2
  apiVersion: v2
  ## Service account for Alertmanager to use.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  serviceAccount:
    create: true
    name: ""
    annotations: {}
    automountServiceAccountToken: true
  ## Configure pod disruption budgets for Alertmanager
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget
  ## This configuration is immutable once created and will require the PDB to be deleted to be changed
  ## https://github.com/kubernetes/kubernetes/issues/45398
  podDisruptionBudget:
    enabled: false
    minAvailable: 1
    maxUnavailable: ""
  ## Alertmanager configuration directives
  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
  config:
    global:
      resolve_timeout: 5m
    inhibit_rules:
      - source_matchers:
          - 'severity = critical'
        target_matchers:
          - 'severity =~ warning|info'
        equal:
          - 'namespace'
          - 'alertname'
      - source_matchers:
          - 'severity = warning'
        target_matchers:
          - 'severity = info'
        equal:
          - 'namespace'
          - 'alertname'
      - source_matchers:
          - 'alertname = InfoInhibitor'
        target_matchers:
          - 'severity = info'
        equal:
          - 'namespace'
      - target_matchers:
          - 'alertname = InfoInhibitor'
    receivers:
      - name: empty
      - name: alertmanager-bot
        webhook_configs:
          - send_resolved: true
            url: "http://alertmanager-bot:8080"
      - name: 'null'
    templates:
      - '/etc/alertmanager/config/*.tmpl'
    route:
      group_by: ["namespace"]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: alertmanager-bot
      routes:
        - matchers:
            - alertname=~"etcdInsufficientMembers|Watchdog|KubeProxyDown|KubeSchedulerDown|KubeControllerManagerDown"
          receiver: empty
        - matchers:
            - alertname="TargetDown"
            - job=~"kube-controller-manager|kube-etcd|kube-proxy|kube-scheduler"
          receiver: empty
        - matchers:
            - alertname="PrometheusOperatorRejectedResources"
          receiver: empty
        - matchers:
            - alertname = "Watchdog"
          receiver: empty
  ## Alertmanager configuration directives (as string type, preferred over the config hash map)
  ## stringConfig will be used only, if tplConfig is true
  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
  stringConfig: ""
  ## Pass the Alertmanager configuration directives through Helm's templating
  ## engine. If the Alertmanager configuration contains Alertmanager templates,
  ## they'll need to be properly escaped so that they are not interpreted by
  ## Helm
  ## ref: https://helm.sh/docs/developing_charts/#using-the-tpl-function
  ##      https://prometheus.io/docs/alerting/configuration/#tmpl_string
  ##      https://prometheus.io/docs/alerting/notifications/
  ##      https://prometheus.io/docs/alerting/notification_examples/
  tplConfig: false
  ## Alertmanager template files to format alerts
  ## By default, templateFiles are placed in /etc/alertmanager/config/ and if
  ## they have a .tmpl file suffix will be loaded. See config.templates above
  ## to change, add other suffixes. If adding other suffixes, be sure to update
  ## config.templates above to include those suffixes.
  ## ref: https://prometheus.io/docs/alerting/notifications/
  ##      https://prometheus.io/docs/alerting/notification_examples/
  templateFiles: {}
  ## An example template:
  #   template_1.tmpl: |-
  #       {{ define "cluster" }}{{ .ExternalURL | reReplaceAll ".*alertmanager\\.(.*)" "$1" }}{{ end }}
  #
  #       {{ define "slack.myorg.text" }}
  #       {{- $root := . -}}
  #       {{ range .Alerts }}
  #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
  #         *Cluster:* {{ template "cluster" $root }}
  #         *Description:* {{ .Annotations.description }}
  #         *Graph:* <{{ .GeneratorURL }}|:chart_with_upwards_trend:>
  #         *Runbook:* <{{ .Annotations.runbook }}|:spiral_note_pad:>
  #         *Details:*
  #           {{ range .Labels.SortedPairs }} - *{{ .Name }}:* `{{ .Value }}`
  #           {{ end }}
  #       {{ end }}
  #       {{ end }}
  ingress:
    enabled: false
    annotations: {}
    labels: {}
    hosts: []
      # - alertmanager.domain.com
    paths: []
    # - /
    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
    # pathType: ImplementationSpecific
    tls: []
    # - secretName: alertmanager-general-tls
    #   hosts:
    #   - alertmanager.example.com

  ## Configuration for Alertmanager secret
  ##
  secret:
    annotations: {}

  ## Configuration for creating an Ingress that will map to each Alertmanager replica service
  ## alertmanager.servicePerReplica must be enabled
  ##
  ingressPerReplica:
    enabled: false

    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx

    annotations: {}
    labels: {}

    ## Final form of the hostname for each per replica ingress is
    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
    ##
    ## Prefix for the per replica ingress that will have `-$replicaNumber`
    ## appended to the end
    hostPrefix: ""
    ## Domain that will be used for the per replica ingress
    hostDomain: ""
    ## Paths to use for ingress rules
    ##
    paths: []
    # - /
    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
    # pathType: ImplementationSpecific
    ## Secret name containing the TLS certificate for alertmanager per replica ingress
    ## Secret must be manually created in the namespace
    tlsSecretName: ""
    ## Separated secret for each per replica Ingress. Can be used together with cert-manager
    ##
    tlsSecretPerReplica:
      enabled: false
      ## Final form of the secret for each per replica ingress is
      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
      ##
      prefix: "alertmanager"
  ## Configuration for Alertmanager service
  ##
  service:
    annotations: {}
    labels: {}
    clusterIP: ""
    ## Port for Alertmanager Service to listen on
    port: 9093
    ## To be used with a proxy extraContainer port
    targetPort: 9093
    ## Port to expose on each node
    ## Only used if service.type is 'NodePort'
    nodePort: 30903
    ## List of IP addresses at which the Prometheus server service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ## Additional ports to open for Alertmanager service
    additionalPorts: []
    # - name: oauth-proxy
    #   port: 8081
    #   targetPort: 8081
    # - name: oauth-metrics
    #   port: 8082
    #   targetPort: 8082
    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
    externalTrafficPolicy: Cluster
    ## If you want to make sure that connections from a particular client are passed to the same Pod each time
    ## Accepts 'ClientIP' or 'None'
    sessionAffinity: None
    ## If you want to modify the ClientIP sessionAffinity timeout
    ## The value must be >0 && <=86400(for 1 day) if ServiceAffinity == "ClientIP"
    sessionAffinityConfig:
      clientIP:
        timeoutSeconds: 10800
    ## Service type
    type: ClusterIP
  ## Configuration for creating a separate Service for each statefulset Alertmanager replica
  servicePerReplica:
    enabled: false
    annotations: {}
    ## Port for Alertmanager Service per replica to listen on
    port: 9093
    ## To be used with a proxy extraContainer port
    targetPort: 9093
    ## Port to expose on each node
    ## Only used if servicePerReplica.type is 'NodePort'
    nodePort: 30904
    ## Loadbalancer source IP ranges
    ## Only used if servicePerReplica.type is "LoadBalancer"
    loadBalancerSourceRanges: []
    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
    externalTrafficPolicy: Cluster
    ## Service type
    type: ClusterIP
  ## Configuration for creating a ServiceMonitor for AlertManager
  serviceMonitor:
    ## If true, a ServiceMonitor will be created for the AlertManager service.
    selfMonitor: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""
    ## Additional labels
    additionalLabels: {}
    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    sampleLimit: 0
    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    targetLimit: 0
    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelLimit: 0
    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelNameLengthLimit: 0
    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelValueLengthLimit: 0
    ## proxyUrl: URL of a proxy that should be used for scraping.
    proxyUrl: ""
    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
    scheme: ""
    ## enableHttp2: Whether to enable HTTP2.
    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint
    enableHttp2: true
    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
    tlsConfig: {}
    bearerTokenFile:
    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]
    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    relabelings:
      # Replace job value
      - sourceLabels:
        - __address__
        action: replace
        targetLabel: job
        replacement: alertmanager
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace
    ## Additional Endpoints
    additionalEndpoints: []
    # - port: oauth-metrics
    #   path: /metrics
  ## Settings affecting alertmanagerSpec
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec
  alertmanagerSpec:
    # configMaps:
    #   - alertmanager-telegram-template
    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: alertmanager
    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
    ## Metadata Labels and Annotations gets propagated to the Alertmanager pods.
    podMetadata: {}
    ## Image of Alertmanager
    # image:
    #   registry: quay.io
    #   repository: prometheus/alertmanager
    #   tag: v0.27.0
    #   sha: ""
    ## If true then the user will be responsible to provide a secret with alertmanager configuration
    ## So when true the config part will be ignored (including templateFiles) and the one in the secret will be used
    useExistingSecret: false
    ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the
    ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.
    ##
    secrets: []
    ## If false then the user will opt out of automounting API credentials.
    automountServiceAccountToken: true
    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.
    ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.
    configMaps: []
    ## ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for
    ## this Alertmanager instance. Defaults to 'alertmanager-' The secret is mounted into /etc/alertmanager/config.
    configSecret: alertmanager-secret
    ## WebTLSConfig defines the TLS parameters for HTTPS
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerwebspec
    web: {}
    ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with.
    alertmanagerConfigSelector: {}
    ## Example which selects all alertmanagerConfig resources
    ## with label "alertconfig" with values any of "example-config" or "example-config-2"
    # alertmanagerConfigSelector:
    #   matchExpressions:
    #     - key: alertconfig
    #       operator: In
    #       values:
    #         - example-config
    #         - example-config-2
    ## Example which selects all alertmanagerConfig resources with label "role" set to "example-config"
    # alertmanagerConfigSelector:
    #   matchLabels:
    #     role: example-config
    ## Namespaces to be selected for AlertmanagerConfig discovery. If nil, only check own namespace.
    alertmanagerConfigNamespaceSelector: {}
    ## Example which selects all namespaces
    ## with label "alertmanagerconfig" with values any of "example-namespace" or "example-namespace-2"
    # alertmanagerConfigNamespaceSelector:
    #   matchExpressions:
    #     - key: alertmanagerconfig
    #       operator: In
    #       values:
    #         - example-namespace
    #         - example-namespace-2
    ## Example which selects all namespaces with label "alertmanagerconfig" set to "enabled"
    # alertmanagerConfigNamespaceSelector:
    #   matchLabels:
    #     alertmanagerconfig: enabled
    ## AlermanagerConfig to be used as top level configuration
    alertmanagerConfiguration: {}
    ## Example with select a global alertmanagerconfig
    # alertmanagerConfiguration:
    #   name: global-alertmanager-Configuration
    ## Defines the strategy used by AlertmanagerConfig objects to match alerts. eg:
    alertmanagerConfigMatcherStrategy: {}
    ## Example with use OnNamespace strategy
    # alertmanagerConfigMatcherStrategy:
    #   type: OnNamespace
    ## Define Log Format
    # Use logfmt (default) or json logging
    logFormat: logfmt
    ## Log level for Alertmanager to be configured with.
    logLevel: info
    ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the
    ## running cluster equal to the expected size.
    replicas: 1
    ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression
    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
    retention: 120h
    ## Storage is the definition of how storage will be used by the Alertmanager instances.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: local-path
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 1Gi
    ## The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name. string  false
    externalUrl: https://alertmanager.anotherlife.pl/
    ## The route prefix Alertmanager registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,
    ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.
    routePrefix: /
    ## scheme: HTTP scheme to use. Can be used with `tlsConfig` for example if using istio mTLS.
    scheme: ""
    ## tlsConfig: TLS configuration to use when connect to the endpoint. For example if using istio mTLS.
    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
    tlsConfig: {}
    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.
    paused: false
    ## Define which Nodes the Pods are scheduled on.
    ## ref: https://kubernetes.io/docs/user-guide/node-selection/
    nodeSelector: {}
    ## Define resources requests and limits for single Pods.
    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
    resources: {}
    # requests:
    #   memory: 400Mi
    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
    ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
    ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
    ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
    podAntiAffinity: ""
    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
    ##
    podAntiAffinityTopologyKey: kubernetes.io/hostname
    ## Assign custom affinity rules to the alertmanager instance
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    affinity: {}
    # nodeAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #     - matchExpressions:
    #       - key: kubernetes.io/e2e-az-name
    #         operator: In
    #         values:
    #         - e2e-az1
    #         - e2e-az2
    ## If specified, the pod's tolerations.
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []
    # - key: "key"
    #   operator: "Equal"
    #   value: "value"
    #   effect: "NoSchedule"
    ## If specified, the pod's topology spread constraints.
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
    topologySpreadConstraints: []
    # - maxSkew: 1
    #   topologyKey: topology.kubernetes.io/zone
    #   whenUnsatisfiable: DoNotSchedule
    #   labelSelector:
    #     matchLabels:
    #       app: alertmanager
    ## SecurityContext holds pod-level security attributes and common container settings.
    ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
    securityContext:
      runAsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
      fsGroup: 2000
      seccompProfile:
        type: RuntimeDefault
    ## ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP.
    ## Note this is only for the Alertmanager UI, not the gossip communication.
    listenLocal: false
    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod.
    containers: []
    # containers:
    # - name: oauth-proxy
    #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1
    #   args:
    #   - --upstream=http://127.0.0.1:9093
    #   - --http-address=0.0.0.0:8081
    #   - --metrics-address=0.0.0.0:8082
    #   - ...
    #   ports:
    #   - containerPort: 8081
    #     name: oauth-proxy
    #     protocol: TCP
    #   - containerPort: 8082
    #     name: oauth-metrics
    #     protocol: TCP
    #   resources: {}
    # Additional volumes on the output StatefulSet definition.
    volumes: []
    # Additional VolumeMounts on the output StatefulSet definition.
    volumeMounts: []
    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
    ## (permissions, dir tree) on mounted volumes before starting prometheus
    initContainers: []
    ## Priority class assigned to the Pods
    priorityClassName: ""
    ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.
    additionalPeers: []
    ## PortName to use for Alert Manager.
    portName: "http-web"
    ## ClusterAdvertiseAddress is the explicit address to advertise in cluster. Needs to be provided for non RFC1918 [1] (public) addresses. [1] RFC1918: https://tools.ietf.org/html/rfc1918
    clusterAdvertiseAddress: false
    ## clusterGossipInterval determines interval between gossip attempts.
    ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)
    clusterGossipInterval: ""

    ## clusterPeerTimeout determines timeout for cluster peering.
    ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)
    clusterPeerTimeout: ""

    ## clusterPushpullInterval determines interval between pushpull attempts.
    ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)
    clusterPushpullInterval: ""

    ## ForceEnableClusterMode ensures Alertmanager does not deactivate the cluster mode when running with a single replica.
    ## Use case is e.g. spanning an Alertmanager cluster across Kubernetes clusters with a single replica in each.
    forceEnableClusterMode: false

    ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to
    ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).
    minReadySeconds: 0

    ## Additional configuration which is not covered by the properties above. (passed through tpl)
    additionalConfig: {}

    ## Additional configuration which is not covered by the properties above.
    ## Useful, if you need advanced templating inside alertmanagerSpec.
    ## Otherwise, use alertmanager.alertmanagerSpec.additionalConfig (passed through tpl)
    additionalConfigString: ""

  ## ExtraSecret can be used to store various data in an extra secret
  ## (use it for example to store hashed basic auth credentials)
  extraSecret:
    ## if not set, name will be auto generated
    # name: ""
    annotations: {}
    data: {}
  #   auth: |
  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.


## Grafana
## Default values: https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
grafana:
  enabled: true
  fullnameOverride: ""
  deploymentStrategy:
    type: Recreate
  revisionHistoryLimit: 4
  serviceMonitor:
    scrapeTimeout: 5s

  namespaceOverride: ""
  ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled
  forceDeployDatasources: false
  ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled
  forceDeployDashboards: false
  ## Deploy default dashboards
  defaultDashboardsEnabled: true
  ## Timezone for the default dashboards
  defaultDashboardsTimezone: CET
  ## Editable flag for the default dashboards
  defaultDashboardsEditable: true
  adminPassword: prom-operator
  plugins:
    - grafana-piechart-panel

  # needed for loading initial datasources
  # admin:
  #   existingSecret: grafana-admin-secret
  #   userKey: admin-user
  #   passwordKey: admin-password

  grafana.ini:
    server:
      root_url: https://grafana.anotherlife.pl
    paths:
      data: /var/lib/grafana/
      logs: /var/log/grafana
      plugins: /var/lib/grafana/plugins
      provisioning: /etc/grafana/provisioning
    analytics:
      check_for_updates: true
    log:
      mode: console
      level: debug #info
    # auth:
    #   disable_login_form: true
    #   signout_redirect_url: "https://grafana.anotherlife.pl/oauth2/sign_out"
    # auth.basic:
    #   enabled: false
    # auth.proxy:
    #   enabled: true
    #   header_name: X-Auth-Request-Email
    #   header_property: email
    #   auto_sign_up: true
    users:
      allow_sign_up: false
      auto_assign_org: true
      auto_assign_org_role: Admin

    # auth.generic_oauth:
    #   enabled: true
    #   allow_sign_up: true
    #   name: idm.anotherlife.pl
    #   client_id: $__env{GRAFANA_SSO_CLIENT_ID}
    #   client_secret: $__env{GRAFANA_SSO_CLIENT_SECRET}
    #   scopes: openid profile email
    #   auth_url: https://idm.anotherlife.pl/ui/oauth2
    #   token_url: https://idm.anotherlife.pl/oauth2/token
    #   api_url: https://idm.anotherlife.pl/oauth2/openid/grafana/userinfo
    #   use_pkce: true
    #   groups_attribute_path: scopes
    #   name_attribute_path: preferred_username
    #   role_attribute_path: contains(scopes[*], 'admin') && 'Admin' || 'Viewer'

  rbac:
    ## If true, Grafana PSPs will be created
    pspEnabled: false

  ingress:
    enabled: false
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    labels: {}
    # hosts:
    #   - grafana.domain.com
    hosts: []
    path: /
    tls: []
    # - secretName: grafana-general-tls
    #   hosts:
    #   - grafana.example.com

  # # To make Grafana persistent (Using Statefulset)
  persistence:
    enabled: true
    type: sts
    storageClassName: "local-path"
    accessModes:
      - ReadWriteOnce
    size: 10Gi
    finalizers:
      - kubernetes.io/pvc-protection

  # persistence:
  #   enabled: true
  #   size: 10Gi
  #   accessModes:
  #     - ReadWriteOnce
  #   finalizers:
  #     - kubernetes.io/pvc-protection

  serviceAccount:
    create: true
    autoMount: true

  # Organise dashboards by provider, with the provider's name as the key.
  dashboards:
    default: # The "default" provider is defined below.
      nginx-ingress-controller:
        url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.48.1/deploy/grafana/dashboards/nginx.json
      ingress-request-performance:
        url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.48.1/deploy/grafana/dashboards/request-handling-performance.json
    extra:
      pihole-overview:
        gnetId: 10176
        datasource: Prometheus
      argocd-overview:
        gnetId: 14584
        datasource: Prometheus
      cert-manager-overview:
        gnetId: 11001
        datasource: Prometheus
      external-dns-overview:
        gnetId: 15038
        datasource: Prometheus
      blackbox-exporter-overview:
        gnetId: 5345
        datasource: Prometheus
      # thanos-overview:
      #   gnetId: 12937
      #   datasource: Thanos

  # Configure dashboard providers.
  # ref: http://docs.grafana.org/administration/provisioning/#dashboards
  # `path` must be /var/lib/grafana/dashboards/<provider_name>
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: default
          orgId: 1
          folder: ""
          type: file
          disableDeletion: false
          editable: true
          allowUiUpdates: true
          options:
            path: /var/lib/grafana/dashboards/default
        - name: "extra"
          orgId: 1
          folder: "Extra"
          type: file
          disableDeletion: true
          editable: true
          allowUiUpdates: true
          options:
            path: /var/lib/grafana/dashboards/extra
        - name: "nginx"
          orgId: 1
          folder: "nginx"
          type: file
          disableDeletion: true
          editable: true
          allowUiUpdates: true
          options:
            path: /var/lib/grafana/dashboards/nginx


  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          url: http://monitoring-kube-prometheus-prometheus.monitoring:9090 #http://kube-prometheus-stack-prometheus:9090
          access: proxy
          isDefault: true
        - name: Loki
          type: loki
          url: http://loki-stack:3100
          access: proxy
        - name: mqtt
          type: mqtt
          url: tcp://eclipse-mosquitto.iot-k3s:1883
          access: proxy
        # - name: Thanos
        #   type: prometheus
        #   url: http://thanos-query-frontend:9090
        #   access: proxy



######################## Test It
  # inMemory:
  #   enabled: true
  #   ## The maximum usage on memory medium EmptyDir would be
  #   ## the minimum value between the SizeLimit specified
  #   ## here and the sum of memory limits of all containers in a pod
  #   sizeLimit: 256Mi

  #envFromSecret: grafana-secret

#########################


  extraConfigmapMounts: []
  # - name: certs-configmap
  #   mountPath: /etc/grafana/ssl/
  #   configMap: certs-configmap
  #   readOnly: true
  deleteDatasources: []
  # - name: example-datasource
  #   orgId: 1
  ## Configure additional grafana datasources (passed through tpl)
  ## ref: http://docs.grafana.org/administration/provisioning/#datasources
  additionalDataSources: []
  # - name: prometheus-sample
  #   access: proxy
  #   basicAuth: true
  #   basicAuthPassword: pass
  #   basicAuthUser: daco
  #   editable: false
  #   jsonData:
  #       tlsSkipVerify: true
  #   orgId: 1
  #   type: prometheus
  #   url: https://{{ printf "%s-prometheus.svc" .Release.Name }}:9090
  #   version: 1
  ## Passed to grafana subchart and used by servicemonitor below
  service:
    portName: http-web

  serviceMonitor:
    enabled: true
    path: /metrics
    labels:
      release: monitoring
    interval: ""
    scheme: http
    tlsConfig: {}
    scrapeTimeout: 30s
    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

## Flag to disable all the kubernetes component scrapers
kubernetesServiceMonitors:
  enabled: true
## Component scraping the kube api server
kubeApiServer:
  enabled: true
  tlsConfig:
    serverName: kubernetes
    insecureSkipVerify: false
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""
    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    sampleLimit: 0
    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    targetLimit: 0
    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelLimit: 0
    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelNameLengthLimit: 0
    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelValueLengthLimit: 0
    ## proxyUrl: URL of a proxy that should be used for scraping.
    proxyUrl: ""
    jobLabel: component
    selector:
      matchLabels:
        component: apiserver
        provider: kubernetes
    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    metricRelabelings:
      # Drop excessively noisy apiserver buckets.
      - action: drop
        regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)
        sourceLabels:
          - __name__
          - le
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]
    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    relabelings: []
    # - sourceLabels:
    #     - __meta_kubernetes_namespace
    #     - __meta_kubernetes_service_name
    #     - __meta_kubernetes_endpoint_port_name
    #   action: keep
    #   regex: default;kubernetes;https
    # - targetLabel: __address__
    #   replacement: kubernetes.default.svc:443
    ## Additional labels
    additionalLabels: {}
    #  foo: bar

## Component scraping the kubelet and kubelet-hosted cAdvisor
kubelet:
  enabled: true
  namespace: kube-system
  serviceMonitor:
    ## Attach metadata to discovered targets. Requires Prometheus v2.45 for endpoints created by the operator.
    attachMetadata:
      node: false
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""
    ## If true, Prometheus use (respect) labels provided by exporter.
    honorLabels: true
    ## If true, Prometheus ingests metrics with timestamp provided by exporter. If false, Prometheus ingests metrics with timestamp of scrape.
    honorTimestamps: true
    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    sampleLimit: 0
    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    targetLimit: 0
    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelLimit: 0
    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelNameLengthLimit: 0
    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelValueLengthLimit: 0
    ## proxyUrl: URL of a proxy that should be used for scraping.
    proxyUrl: ""
    ## Enable scraping the kubelet over https. For requirements to enable this see
    ## https://github.com/prometheus-operator/prometheus-operator/issues/926
    https: false #true
    ## Enable scraping /metrics/cadvisor from kubelet's service
    cAdvisor: true
    ## Enable scraping /metrics/probes from kubelet's service
    probes: true
    ## Enable scraping /metrics/resource from kubelet's service
    ## This is disabled by default because container metrics are already exposed by cAdvisor
    resource: false
    # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource
    resourcePath: "/metrics/resource/v1alpha1"
    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    cAdvisorMetricRelabelings:
      # Drop less useful container CPU metrics.
      - sourceLabels: [__name__]
        action: drop
        regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'
      # Drop less useful container / always zero filesystem metrics.
      - sourceLabels: [__name__]
        action: drop
        regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'
      # Drop less useful / always zero container memory metrics.
      - sourceLabels: [__name__]
        action: drop
        regex: 'container_memory_(mapped_file|swap)'
      # Drop less useful container process metrics.
      - sourceLabels: [__name__]
        action: drop
        regex: 'container_(file_descriptors|tasks_state|threads_max)'
      # Drop container spec metrics that overlap with kube-state-metrics.
      - sourceLabels: [__name__]
        action: drop
        regex: 'container_spec.*'
      # Drop cgroup metrics with no pod.
      - sourceLabels: [id, pod]
        action: drop
        regex: '.+;'
    # - sourceLabels: [__name__, image]
    #   separator: ;
    #   regex: container_([a-z_]+);
    #   replacement: $1
    #   action: drop
    # - sourceLabels: [__name__]
    #   separator: ;
    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
    #   replacement: $1
    #   action: drop

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    probesMetricRelabelings: []
    # - sourceLabels: [__name__, image]
    #   separator: ;
    #   regex: container_([a-z_]+);
    #   replacement: $1
    #   action: drop
    # - sourceLabels: [__name__]
    #   separator: ;
    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
    #   replacement: $1
    #   action: drop

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ## metrics_path is required to match upstream rules and charts
    cAdvisorRelabelings:
      - action: replace
        sourceLabels: [__metrics_path__]
        targetLabel: metrics_path
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    probesRelabelings:
      - action: replace
        sourceLabels: [__metrics_path__]
        targetLabel: metrics_path
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    resourceRelabelings:
      - action: replace
        sourceLabels: [__metrics_path__]
        targetLabel: metrics_path
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    metricRelabelings:
      - sourceLabels:
          - node
        action: replace
        targetLabel: instance
    # - sourceLabels: [__name__, image]
    #   separator: ;
    #   regex: container_([a-z_]+);
    #   replacement: $1
    #   action: drop
    # - sourceLabels: [__name__]
    #   separator: ;
    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
    #   replacement: $1
    #   action: drop

    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ## metrics_path is required to match upstream rules and charts
    relabelings:
      - action: replace
        sourceLabels: [__metrics_path__]
        targetLabel: metrics_path
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

    ## Additional labels
    ##
    additionalLabels: {}
    #  foo: bar

## Component scraping the kube controller manager
kubeControllerManager:
  enabled: true
  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on
  endpoints:
  - 192.168.40.119
  # - 10.141.4.23
  # - 10.141.4.24

  ## If using kubeControllerManager.endpoints only the port and targetPort are used
  service:
    enabled: true
    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change
    ## of default port in Kubernetes 1.22.
    port: null
    targetPort: null
    # selector:
    #   component: kube-controller-manager
  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""
    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    sampleLimit: 0
    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    targetLimit: 0
    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelLimit: 0
    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelNameLengthLimit: 0
    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelValueLengthLimit: 0
    ## proxyUrl: URL of a proxy that should be used for scraping.
    proxyUrl: ""
    ## port: Name of the port the metrics will be scraped from
    port: http-metrics
    jobLabel: jobLabel
    selector:
      matchLabels:
        component: kube-controller-manager
    ## Enable scraping kube-controller-manager over https.
    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.
    ## If null or unset, the value is determined dynamically based on target Kubernetes version.
    https: null
    # Skip TLS certificate validation when scraping
    insecureSkipVerify: null
    # Name of the server to use when validating TLS certificate
    serverName: null
    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]
    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace
    ## Additional labels
    additionalLabels: {}
    #  foo: bar

## Component scraping coreDns. Use either this or kubeDns
coreDns:
  enabled: true
  service:
    enabled: true
    port: 9153
    targetPort: 9153
    selector:
      k8s-app: kube-dns
  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""
    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    sampleLimit: 0
    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    targetLimit: 0
    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelLimit: 0
    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelNameLengthLimit: 0
    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelValueLengthLimit: 0
    ## proxyUrl: URL of a proxy that should be used for scraping.
    proxyUrl: ""
    ## port: Name of the port the metrics will be scraped from
    port: http-metrics
    jobLabel: jobLabel
    selector:
      matchLabels:
        k8s-app: kube-dns
    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]
    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace
    ## Additional labels
    additionalLabels: {}
    #  foo: bar

## Component scraping kubeDns. Use either this or coreDns
kubeDns:
  enabled: false
  service:
    dnsmasq:
      port: 10054
      targetPort: 10054
    skydns:
      port: 10055
      targetPort: 10055
    selector:
      k8s-app: kube-dns
  serviceMonitor:
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""
    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    sampleLimit: 0
    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    targetLimit: 0
    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelLimit: 0
    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelNameLengthLimit: 0
    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelValueLengthLimit: 0
    ## proxyUrl: URL of a proxy that should be used for scraping.
    proxyUrl: ""
    jobLabel: jobLabel
    selector:
      matchLabels:
        k8s-app: kube-dns
    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]
    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace
    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    dnsmasqMetricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]
    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    dnsmasqRelabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace
    ## Additional labels
    additionalLabels: {}
    #  foo: bar

## Component scraping etcd
kubeEtcd:
  enabled: true
  ## If your etcd is not deployed as a pod, specify IPs it can be found on
  endpoints:
    - 192.168.40.119
  # - 10.141.4.23
  # - 10.141.4.24
  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used
  service:
    enabled: true
    port: 2381
    targetPort: 2381
    selector:
      component: etcd

  ## Configure secure access to the etcd cluster by loading a secret into prometheus and
  ## specifying security configuration below. For example, with a secret named etcd-client-cert
  # serviceMonitor:
  #   scheme: https
  #   insecureSkipVerify: false
  #   serverName: localhost
  #   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
  #   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
  #   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
  serviceMonitor:
    enabled: true
    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""
    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
    sampleLimit: 0
    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
    targetLimit: 0
    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelLimit: 0
    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelNameLengthLimit: 0
    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
    labelValueLengthLimit: 0
    ## proxyUrl: URL of a proxy that should be used for scraping.
    proxyUrl: ""
    scheme: http
    insecureSkipVerify: false
    serverName: ""
    caFile: ""
    certFile: ""
    keyFile: ""
    ## port: Name of the port the metrics will be scraped from
    port: http-metrics
    jobLabel: jobLabel
    selector:
      matchLabels:
        component: etcd
    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]
    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace
    ## Additional labels
    additionalLabels: {}
    #  foo: bar



###################################################




kubeScheduler:
  enabled: true
  endpoints: # ips of servers
    - 192.168.40.110
    # - 192.168.40.111
    # - 192.168.40.112

kubeProxy:
  enabled: true
  endpoints: # ips of servers
    - 192.168.40.110
    # - 192.168.40.111
    # - 192.168.40.112

kube-state-metrics:
  fullnameOverride: kube-state-metrics
  selfMonitor:
    enabled: true
  metricLabelsAllowlist:
  - "persistentvolumeclaims=[*]"
  prometheus:
    monitor:
      enabled: true
      relabelings:
        - action: replace
          regex: (.*)
          replacement: $1
          sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: kubernetes_node

nodeExporter:
  enabled: true
  serviceMonitor:
    relabelings:
      - action: replace
        regex: (.*)
        replacement: $1
        sourceLabels:
          - __meta_kubernetes_pod_node_name
        targetLabel: kubernetes_node




prometheusOperator:
  admissionWebhooks:
    enabled: true
  tls:
    enabled: false
  # prometheusConfigReloader:
  #   resources:
  #     requests:
  #       cpu: 200m
  #       memory: 50Mi
  #     limits:
  #       memory: 100Mi

# prometheus-node-exporter:
#   prometheus:
#     monitor:
#       enabled: true
#       relabelings:
#         - action: replace
#           sourceLabels:
#             - __meta_kubernetes_pod_node_name
#           targetLabel: instance
# ## smart disk data metrics
# extraArgs:
#   - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
#   - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
#   # this is the new line
#   - --collector.textfile.directory=/host/root/var/log/prometheus


prometheus-node-exporter:
  fullnameOverride: node-exporter
  podLabels:
    jobLabel: node-exporter
  extraArgs:
    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
    - --collector.textfile.directory=/host/root/var/log/prometheus

  service:
    portName: http-metrics
  prometheus:
    monitor:
      enabled: true
      relabelings:
        - action: replace
          regex: (.*)
          replacement: $1
          sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: kubernetes_node
  # resources:
  #   requests:
  #     memory: 512Mi
  #     cpu: 250m
  #   limits:
  #     memory: 2048Mi

prometheus:
  prometheusSpec:
    enableAdminAPI: true
    replicas: 1
    replicaExternalLabelName: "replica"
    scrapeInterval: 15s
    scrapeConfigSelectorNilUsesHelmValues: true
    retentionSize: 40GB
    retention: 96d
    ruleSelectorNilUsesHelmValues: true
    # Watch all PrometheusRules in the cluster.
    ruleNamespaceSelector:
      matchLabels: {}
    ruleSelector:
      matchLabels: {}
    # Watch all ServiceMonitors in the cluster.
    serviceMonitorNamespaceSelector:
      matchLabels: {}
    serviceMonitorSelector:
      matchLabels: {}
    serviceMonitorSelectorNilUsesHelmValues: true
    # Watch all PodMonitors in the cluster.
    podMonitorSelector:
      matchLabels: {}
    podMonitorNamespaceSelector:
      matchLabels: {}
    podMonitorSelectorNilUsesHelmValues: true
    probeSelectorNilUsesHelmValues: true
    walCompression: true
    # resources:
    #   requests:
    #     cpu: 1
    #     memory: 1Gi
    #   limits:
    #     cpu: 2
    #     memory: 2Gi
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: local-path #longhorn-retain
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 40Gi
    additionalScrapeConfigs:
      - job_name: 'asusrouter'
        scrape_interval: 5s
        static_configs:
          - targets: ['router.asus.com:9101']


    thanos:
      {}
      # objectStorageConfig:
      #   key: objstore.yml
      #   name: thanos-objstore-secret
  ingress:
    enabled: false
    # ingressClassName: nginx
    # annotations:
    #   cert-manager.io/cluster-issuer: letsencrypt
    #   external-dns.alpha.kubernetes.io/target: monitoring.anotherlife.pl
    #   nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth
    #   nginx.ingress.kubernetes.io/auth-signin: https://$host/oauth2/start?rd=$escaped_request_uri
    # hosts:
    #   - prometheus.local
    # tls:
    #   - secretName: prometheus-general-tls
    #     hosts:
    #       - prometheus.local

  thanosService:
    enabled: false

  thanosServiceMonitor:
    enabled: false