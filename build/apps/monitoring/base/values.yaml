defaultRules:
  rules:
    etcd: false
    # Disable until fixing: https://github.com/prometheus-community/helm-charts/issues/1283
    kubeApiserve: false
    kubeProxy: false

alertmanager:
  alertmanagerSpec:
    replicas: 1

    useExistingSecret: true

    # configMaps:
    #   - alertmanager-telegram-template

    configSecret: alertmanager-secret

    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: alertmanager
    # resources:
    #   requests:
    #     cpu: 10m
    #     memory: 40Mi
    #   limits:
    #     cpu: 200m
    #     memory: 250Mi

  # ingress:
  #   enabled: true
  #   ingressClassName: nginx-internal
  #   annotations:
  #     cert-manager.io/cluster-issuer: letsencrypt-prod-dns
  #     external-dns.alpha.kubernetes.io/enabled: "true"
  #   hosts:
  #     - alertmanager.internal.grigri.cloud
  #   paths:
  #     - /
  #   tls:
  #     - secretName: alertmanager-general-tls
  #       hosts:
  #         - alertmanager.internal.grigri.cloud


grafana:
  enabled: true

  defaultDashboardsTimezone: cet

  # needed for loading initial datasources
  # admin:
  #   existingSecret: grafana-admin-secret
  #   userKey: username
  #   passwordKey: password


  plugins:
    - grafana-piechart-panel

  persistence:
    enabled: false
  inMemory:
    enabled: true
    ## The maximum usage on memory medium EmptyDir would be
    ## the minimum value between the SizeLimit specified
    ## here and the sum of memory limits of all containers in a pod
    ##
    sizeLimit: 256Mi

  additionalDataSources:
    - name: Loki
      type: loki
      url: http://loki.loki:3100
      access: proxy
      isDefault: false
      version: 1

  sidecar:
    # resources:
    #   limits:
    #     memory: 128Mi
    #   requests:
    #     cpu: 10m
    #     memory: 64Mi

    dashboards:
      enabled: true
      label: grafana_dashboard
      resource: configmap
      folderAnnotation: grafana.grafana.com/dashboards.target-directory
      provider:
        foldersFromFilesStructure: true
      annotations:
        grafana.grafana.com/dashboards.target-directory: "/tmp/dashboards/kubernetes"
      searchNamespace: ALL


  custom-dashboard:
    # This is a path to a file inside the dashboards directory inside the chart directory
    file: dashboards/router.json

    # extra:
    #   pihole-overview:
    #     gnetId: 10176
    #     datasource: Prometheus
    #   argocd-overview:
    #     gnetId: 14584
    #     datasource: Prometheus
    #   cert-manager-overview:
    #     gnetId: 11001
    #     datasource: Prometheus
    #   external-dns-overview:
    #     gnetId: 15038
    #     datasource: Prometheus
    #   blackbox-exporter-overview:
    #     gnetId: 5345
    #     datasource: Prometheus


  #envFromSecret: grafana-secret

  grafana.ini:
    server:
      root_url: https://grafana.anotherlife.pl

    log:
      level: info

    # auth.generic_oauth:
    #   enabled: true
    #   allow_sign_up: true
    #   name: idm.grigri.cloud
    #   client_id: $__env{GRAFANA_SSO_CLIENT_ID}
    #   client_secret: $__env{GRAFANA_SSO_CLIENT_SECRET}
    #   scopes: openid profile email
    #   auth_url: https://idm.grigri.cloud/ui/oauth2
    #   token_url: https://idm.grigri.cloud/oauth2/token
    #   api_url: https://idm.grigri.cloud/oauth2/openid/grafana/userinfo
    #   use_pkce: true
    #   groups_attribute_path: scopes
    #   name_attribute_path: preferred_username
    #   role_attribute_path: contains(scopes[*], 'admin') && 'Admin' || 'Viewer'

  revisionHistoryLimit: 2

  serviceMonitor:
    enabled: true
    path: /metrics
    labels:
      release: monitoring

coreDns:
  enabled: true
  service:
    selector:
      k8s-app: kube-dns

kubeEtcd:
  enabled: false

kubeScheduler:
  enabled: false

kubeProxy:
  enabled: false

kubeControllerManager:
  enabled: false

prometheusOperator:
  # https://github.com/helm/charts/issues/19147
  admissionWebhooks:
    enabled: false
  tls:
    enabled: false

  # resources:
  #   limits:
  #     memory: 200Mi
  #   requests:
  #     cpu: 10m
  #     memory: 100Mi

prometheus-node-exporter:
  prometheus:
    monitor:
      enabled: true
      relabelings:
        - action: replace
          sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: instance
## smart disk data metrics
extraArgs:
  - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
  - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
  # this is the new line
  - --collector.textfile.directory=/host/root/var/log/prometheus

prometheus:
  prometheusSpec:
    replicas: 1
    retentionSize: 3GB
    retention: 7d
    # resources:
    #   requests:
    #     cpu: 1
    #     memory: 2Gi
    #   limits:
    #     cpu: 4
    #     memory: 2.5Gi

    # Change this to false doesn't require label `release: monitoring` for following resources:
    ruleSelectorNilUsesHelmValues: true
    serviceMonitorSelectorNilUsesHelmValues: true
    podMonitorSelectorNilUsesHelmValues: true
    probeSelectorNilUsesHelmValues: true
    scrapeConfigSelectorNilUsesHelmValues: true

    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn-retain #local-path #zfs-local-dataset
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 5Gi

